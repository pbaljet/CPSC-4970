# Software Requirements Specification

## Purpose

Data generated from IT automation is becoming increasingly important to companies.  This data is used to monitor business processes, improve automation cost and effectiveness, meet compliance requirements, and provide competitive advantages.  The data generated needs to be easily captured and in a flexible structure as data records and elements can not be anticipated ahead of time.  Additionally, captured data needs to be easily exported using different methods to external storage technologies and observability tools.

Providing the ability to capture, store, forward, and analyze data provides customers the flexibility to integrate advanced analytics and monitoring capabilities on top of UAC.  UAC is not a data observability tool and is not analytics engine.  However, it is vital  UAC provide the capability to enable these tools as IT automation plays a critical role in a companies operations.

In modern companies automation is deployed in several key domains: 

- Workload Automation
    * Mainframe
    * Hybrid IT (On Premis, Cloud Native, Cloud, SaaS)
- Cloud Orchestration
    * Public Cloud (AWS, Azure, GCP)
    * Private Cloud (Hypervisors)
- Data Operations (Pi)
    * Data Pipelines
    * Data Movement
- DevOps - mainly around deploying dev/test/prod environments

## Long Term Scope

The Data Management scope is focused on the data generated by an individual Task execution, the storage of the data, and forwarding of the data to an external system.  Key aspects:
- Tasks can be core UAC tasks or Extension tasks.  
- Support for standard and optionally custom data.
- Data is structured in an industry standard format.
- Data is can be stored in a variety of technologies.
- Open source tool for dashboard analytics (Grafana?)
- Data can be external transferred to another system (Web Hook?, External DB?)

The initial Use Cases to drive the design are:
- Data Transfer (UDM, UDMG, ICDT, UFTP)
- Data Pipelines feeding Data Observability tool
- Automation Metrics feeding performance dashboard 

It is not expected all features and functionality are provided in a single UAC release.  The initial versions focus on the building blocks of defining and generating data and a design providing the flexibility in storing and exporting data as new technologies emerge in the areas of storage, analytics, dashboards, observability, and AI.

## UAC 7.5 Focus

- Define standard data structure and conventions
    * Time series data (data pipeline, automation metrics)
    * Tabular data (data transfer record)
- Define exporter 


## Definitions

| Term/Abbreviation | Definition                  |
|-------------------|-----------------------------|
| Task              | UAC core or extension task. |
| Data Package      | Data generated by a task execution in JSON format |


## Overview

Data Flow and Analytics is provided in figure 1. The 3rd party technologies are not decided and for conceptual purposes:

![Figure 1. Overview Diagram](Analytics_Module_Overview.png)

The process flow is broken into the components below each with corresponding detailed sections.

1. **[Definition of standard Data package](#1-standard-data-package)**
2. **[Data package generated from task execution](#2-task-execution)**
4. **[Storage in UAC database](#4-storage-in-uc-database)**
5. **[Load to Analytics storage](#5-transfer-to-external-system)**
6. **[Export to external system](#6-transfer-to-external-system)**
7. **[Data Visualization](#6-transfer-to-external-system)**

### 1. Standard Data Package

The data generated from task execution contains:
- Standard data elements (mostly from ops_exec)
    * Basic Elements
      * Start Time
      * End Time
      * Duration
      * Task Id
      * Task Name
      * Exit Code
      * Workflow ID
      * Variables (optionally selected?)
    * Extended Elements
      * All Basic Elements
      * Queued Time
      * CPU Time
      * Priority
      * Trigger ID
      * Retry Counter
      * Launch Time
      * Trigger Time

Custom data elements come from name/value pairs defined at the tas
- Custom data elements (JSON package)
  * Added to ExtensionResult payload
  * Added to Event response payload
  * Scripts can pass JSON payload through standard out?
  * Payload 
    * Metric Payload 
      * Dimensions - list of strings
      * Name
      * Value - String, Integer, Float
      * Timestamp value collected
      * Threshold
    * Record Payload
      * Name/Value pairs

The difference between Metric and Record payload is to differentiate between the collection of metrics, which can be aggregated, trended, etc. vs. records for traceability/recording (e.g data transfer)

Agent specific task execution data will be prioritezed:
- UAG
- UDM
- UFTP
- USAP
- PPS
- SOA
- UCMD

#### Data Size Limit

Standard data size limit is to be defined as a system configuration.  A maximum value should be defined (16 MB) .  Data packages are meant to contain transactional data or small data sets (< 1000 records).  They are not meant to move or manipulate large data sets. 

### 2. Task Execution 

- Task data generation is configured at the task definition level.  This allows to limit/thottle the amount of data generated and thus the load on the system.  However, if an extension returns data it will processed regardless of the on/off setting.
- A system setting to turn on data generation by default.
- Option to send task context variables in data package
- Tasks are backwards compatible and work the same if no data is generated from existing responses from Extensions or Standard in/out.

### 4. Storage in UC Database

Current output from tasks is stored in the UAC database table OPS_EXEC_OUTPUT.  Current output from extensions has the type of EXTENSION_VALUE (19).  JSON data type implementations vary across different databases so UAC still stores JSON in a character data type.

#### Data Transfer Use Case

UDM, UDMG, ICDT, UFTP generate a standard data transfer data package record. 

### 5. Load to Analytics Storage

Data needs to be stored in a medium that allows for dashboard and analytics tools to be used.  There are several options for storing the data:
- Generic relational database tables with fixed number of generic columns
- Auto creation of relational tables based on JSON structure
- Manual creation of relational tables and data inserted based on naming conventions
- Export/Push data to time series database (Prometheus)

### 6. Export to External System

Cloud storage technologies and data observability tools exposes data to wide range of analytics tools.  Providing the ability to export data is a key capability to give customers the flexibility to utilize data to meet their business requirements.

- Cloud Storage (utilize intercloud data transfer?)
    * Format - JSON, CSV
    * Object Storage - S3, Azure Blob, GCP  Cloud Storage
    * Database
- Data Observability Tools (Utilizing Events or Webhooks)
    * DataKitchen
    * Prometheus


# Use Cases 

## Centralize Data Transfer

### Overview

Data traceability is increasingly important to companies as they move and process data from various systems in a hybrid IT landscape.  Tracking data movement is important for data privacy, data lineage, industry and govermental compliance and traceability (data breaches). 

### Requirements

| Req                           | Description                                                                                                                                                                                                                                                      |
|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| Standard data transfer record | Any data transfer (file or record set) has a list of defined data elements that are generated for movement of data from one location to the other. Data includes a mechanism to store name/value pairs to allow for non-standard/meta data values (eg data type) |
| UDM Transfer Record           | UDM stores data transfer records in a local database. UDM generates a standard data transfer data packet to be processed.                                                                                                                                        |
| ICDT Transfer Record          | Intercloud Data Transfer extension generates a data transfer record                                                                                                                                                                                              |
| UDMG Transfer Record          | UDMG currently tracks transfer data in it's own database.  This data is standardard to the defined standard and a message generated and sent to the controller.                                                                                                  |
| UFTP Transfer Record          | UFTP generates a data transfer                                                                                                                                                                                                                                   |
| Storing of tranfer records    | Transfer records are expanded and stored in the UAC database                                                                                                                                                                                                     |
 | Export of transfer records    | Data records can be exported based on defined methods (Web Hook, External storage, etc.)                                                                                                                                                                         |
| Provide basic reports         | In the UAC UI there is a standard report with related filters and security controls to view and report on transfer records.                                                                                                                                      |

## Automation Data Analytics

### Overview

Data traceability is increasingly important to companies as they move and process data from various systems in a hybrid IT landscape.  Tracking data movement is important for data privacy, data lineage, industry and govermental compliance and traceability (data breaches).

### Requirements

| Req                           | Description                                                                                                                                                                                                                                                      |
|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|


## Data Pipeline Analytics or Data Observability

### Overview

Data pipelines generate metrics during workflow/DAG step execution.  Data can be related to data quality, performance metrics, data size, etc.  UAC collects this data during extension execution related to data pipeline workflows.  This data is stored and optionally forward to external tools such as Data Observability tools.  The figure below depicts a simple data pipeline and data generated from each step. 

![Figure 1. Data Pipeline](DataPipeline.png)

Data typically collected during pipelines:
- Lineage - connections between generation, transformation, and consumption
- Data Quality
    * Distribution - null values, data ranges
    * Expect min/max rows
- Time based
    * Frequency - how often data is updated.
    * Freshness - data age
    * Timeframe - data time length span
    * Processing duration
- Completeness
    * Missing values
    * Volume
- Schema
    * Schema changes
    * Unexpected data
- Cost
    * Cost per query
    * Storage cost
    * Transfer cost

### Requirements

No business logic

| Req                           | Description                                                                                                                                                                                                                                                      |
|-------------------------------|------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| 
